<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Near Line Data Store (NLDS) &mdash; Near-line Data Store 1.0.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="The System Status Monitor" href="system-status.html" />
    <link rel="prev" title="CEDA Near-Line Data Store" href="home.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/ceda.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="home.html">Getting started</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Specification document</a></li>
<li class="toctree-l1"><a class="reference internal" href="system-status.html">Using the system status page</a></li>
<li class="toctree-l1"><a class="reference internal" href="server-config/server-config.html">The server config file</a></li>
<li class="toctree-l1"><a class="reference internal" href="server-config/examples.html">Server config examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="deployment.html">Deployment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="development/cta-emulator.html">Setting up a CTA tape emulator</a></li>
<li class="toctree-l1"><a class="reference internal" href="development/alembic-migrations.html">Database Migrations with Alembic</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api-reference/nlds-processors.html">NLDS Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-reference/nlds-server.html">NLDS Server</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Near-line Data Store</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Near Line Data Store (NLDS)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/specification.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="near-line-data-store-nlds">
<h1>Near Line Data Store (NLDS)<a class="headerlink" href="#near-line-data-store-nlds" title="Permalink to this headline"></a></h1>
<p># Software specification
<strong>Neil Massey 02/11/2021 -&gt; 29/09/2022</strong></p>
<p># Introduction</p>
<p>As a successor to the Joint Data Migration Application (JDMA), a new storage
solution is proposed.  This is based on the idea of hot, warm and cold storage:</p>
<ul class="simple">
<li><p>hot  = POSIX disk, or SSD.  Expensive in cost and power requirements.</p></li>
<li><p>warm = Object Storage.  Less expensive in cost and power requirements.</p></li>
<li><p>cold = Tape.  Cheapest in cost and power requirements.</p></li>
</ul>
<p>(This is all “in theory” costing.)</p>
<p>The main idea is that users are presented with a single (and simple)
application or API, that follows the CRUD (create, read, update, destroy)
mnemonic.  Users can issue commands to POST a list of files (a list may
contain exactly one file), GET a list of files, DELETE a list of files and PUT
a list of files (update).</p>
<p>This can be via the command line client, or the API that the CLI is built
upon.  Users issue a transfer command and the NLDS system performs the
transfer on their behalf.  For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`nlds</span> <span class="pre">put</span> <span class="pre">&lt;file&gt;`</span></code></p>
<p>puts a single file onto the NLDS system.</p>
<p><code class="docutils literal notranslate"><span class="pre">`nlds</span> <span class="pre">putlist</span> <span class="pre">&lt;filelist&gt;`</span></code></p>
<p>will open the <cite>&lt;filelist&gt;</cite> file, read the file names out of the file and put
those files onto the NLDS.</p>
<p>There are equivalent <cite>nlds get &lt;file&gt;</cite> and <cite>nlds getlist &lt;filelist&gt;</cite> files.
Also, we will have to support <cite>nlds del &lt;file&gt;</cite> and <cite>nlds dellist &lt;filelist&gt;</cite>
commands.</p>
<p>Eventually, we will add monitoring commands as well.</p>
<p>To overcome some of the problems we had with the JDMA, we propose that the
NLDS architecture has a “micro-services” setup.  This consists of:</p>
<ol class="arabic simple">
<li><p>An API server, that clients connect to and issue commands to.  The commands are, as above, the CRUD commands: <cite>put</cite>, <cite>putlist</cite>, <cite>get</cite>, <cite>getlist</cite>, <cite>del</cite>, <cite>dellist</cite>.</p></li>
<li><p>A message-broker queue.  The API server translates the user’s commands to messages and pushes them onto the message-broker queue.</p></li>
<li><p>Micro-service subscribers to the queue.  These micro-services take a message from the queue, perform a task that is encoded in the message, and then push the results back onto the queue for further action.</p></li>
<li><p>A transfer processor.</p></li>
<li><p>A monitoring and notification system.</p></li>
<li><p>A catalogue database, containing the NLDS holdings.</p></li>
</ol>
<p>This basic architecture is shown in Figure 1:</p>
<div class="line-block">
<div class="line">![overview](./uml/overview.png) |</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">-</dt>
<dd class="field-odd"><p></p></dd>
</dl>
<div class="line-block">
<div class="line"><strong>Figure 1</strong> High-level deployment diagram of NLDS. |</div>
</div>
<p># Use cases</p>
<p>## Group Workspace Tape Management</p>
<p><strong>Purpose</strong>: A group workspace is an allocation of storage to a user or group
of users (a project) on JASMIN.  It can use different storage media. There is
a quota for each media type.</p>
<p><strong>Actors:</strong>
1. GWS user
2. GWS Manager
3. CEDA Archive Manager</p>
<p><strong>Entities:</strong>
1. GWS
2. Tape system
3. CEDA Archive</p>
<p><strong>Actions:</strong></p>
<ul class="simple">
<li><p>A user makes an incremental copy</p></li>
<li><p>A user performs a “back-up” (full or incremental) from other storage to tape</p></li>
</ul>
<p>(the first backup is an increment on nothing)
* Someone writes to tape from Archer and then restores onto disk on JASMIN.
* A GWS user needs to make space on their GWS disk.  They write data to tape
and then remove the data from disk that has been copied to tape.
* A user wants to discover and pull data from tape, run an analysis and then
write their results back to tape afterwards.
* Implies metadata scraping on the way in?
* Put data into “time-limited cold storage” for limited period following
completion of project (limit = 18 months TBC).
* Retrieve GWS data from tape ready for it to be incorporated into the CEDA
archive via ingest process.
* GWS Manager can check overall tape usage against quota, for their GWS.</p>
<p>## CEDA Archive</p>
<p><strong>Purpose:</strong> storage management for the CEDA Archive</p>
<p><strong>Actors:</strong>
1. CEDA Archive manager</p>
<p><strong>Entities:</strong>
1. CEDA Archive
2. Tape system</p>
<p><strong>Actions:</strong>
* Ingest.
* Deposit.
* Storage allocation - what data should go where (should be policy driven
rather than list driven).
* Setting up a policy for dataset - e.g. disk-only copy with MODIS.
* Maintenance.
* Make archive copies - exact copy of what is in the archive for redundancy.
* Tidy cache copies - partial copies on performance storage.
* Recovery copies - copies with deleted and modified (Backup).
* Fixity Audit.
* Migration - copy and remove (could be archive, cache or recovery copies).
* Access (either for a download service or direct from mounted file system)
* Search
* Request data to cache copy (NLA type behaviour)</p>
<p># Software components</p>
<p># NLDS client</p>
<ul class="simple">
<li><p>The user interacts with the NLDS client.</p></li>
<li><p>Authorisation tokens are obtained from the OAuth server.</p></li>
<li><p>Commands are issued to the NLDS server, along with the authorisation tokens.</p></li>
<li><p>A transaction ID is generated, which is attached to every message as they</p></li>
</ul>
<p>flow through the system.</p>
<p># NLDS server</p>
<ul class="simple">
<li><p>NLDS client commands are received, along with their authorisation tokens.</p></li>
<li><p>Authorisation tokens are checked with the OAuth server.</p></li>
<li><p>Commands are translated to RabbitMQ commands and pushed to the queue.</p></li>
</ul>
<p>## CRUD operations</p>
<p>These messages are sent to the NLDS server.  These consist of just 6 commands.</p>
<ol class="arabic simple">
<li><p><cite>put</cite> : transfer a single file to the NLDS.</p></li>
<li><p><cite>putlist</cite> : transfer a user-supplied list of files to the NLDS.</p></li>
<li><p><cite>get</cite> : retrieve a single file from the NLDS.</p></li>
<li><p><cite>getlist</cite> : retrieve a user-supplied list of files from the NLDS.</p></li>
<li><p><cite>del</cite> : remove a single file from the NLDS.</p></li>
<li><p><cite>dellist</cite> : remove a user-supplied list of files from the NLDS.</p></li>
</ol>
<p>Each message parameters contain the necessary data to carry out the transaction
on the POSIX filesystem, object storage and tape storage:</p>
<ul class="simple">
<li><p>transaction_id : a unique identifier for each transaction, generated by the nlds client at the point of initiating the transaction.</p></li>
<li><p>user : the user name, used for OAuth2 authentication as well as POSIX filesystem permissions.</p></li>
<li><p>group : the group that the user belongs to and is using to carry out this transaction (users can obviously belong to multiple groups).</p></li>
<li><p>tenancy : the object store tenancy to transfer data to.  This will probably have a default, but the flexibility to PUT / GET data to / from different tenancies might be useful.</p></li>
<li><p>access_key : the user’s access key for the object storage</p></li>
<li><p>secret_key : the user’s secret key for the object storage</p></li>
</ul>
<p>** NOTE ** it is unsatisfactory sending the access_key and secret_key in the parameters, even over HTTPS.  We will probably instigate a service that will exchange a user token for the access key at the point of transfer, using OAuth2.
This will require adding the interface to do so to the object storage.</p>
<p>### PUT command</p>
<div class="line-block">
<div class="line">API endpoint | /files |</div>
</div>
<p><a href="#id25"><span class="problematic" id="id26">|---|</span></a>—|
| HTTP method  | PUT |
| Parameters   | transaction_id |
|              | user |
|              | group |
|              | filepath |
|              | tenancy |
|              | access_key |
|              | secret_key |
| Body         | none |
| Example      | <cite>/files/put?transaction_id=1;user=”bob”;group=”root”;filepath=”myfile.txt”</cite> |</p>
<p>### PUTLIST command</p>
<p>The <a href="#id1"><span class="problematic" id="id2">`</span></a>PUT</p>
<div class="line-block">
<div class="line">API endpoint | /files |</div>
</div>
<p><a href="#id27"><span class="problematic" id="id28">|---|</span></a>—|
| HTTP method  | PUT |
| Parameters   | transaction_id |
|              | user |
|              | group |
|              | tenancy |
|              | access_key |
|              | secret_key |
| Body         | JSON |
| Example      | <a href="#id3"><span class="problematic" id="id4">`</span></a>PUT /files/transaction_id=1;user=”bob”;group=”root”<a href="#id5"><span class="problematic" id="id6">`</span></a>|
| Body example | <a href="#id7"><span class="problematic" id="id8">`</span></a>{“filepath” : [“file1”, “file2”, “file3”]}`|</p>
<p>### GET command</p>
<div class="line-block">
<div class="line">API endpoint | /files |</div>
</div>
<p><a href="#id29"><span class="problematic" id="id30">|---|</span></a>—|
| HTTP method  | GET |
| Parameters   | transaction_id |
|              | user |
|              | group |
|              | filepath |
|              | tenancy |
|              | access_key |
|              | secret_key |
| Body         | none |
| Example      | <cite>GET /files/transaction_id=1;user=”bob”;group=”root”;filepath=”myfile.txt”</cite> |</p>
<p>### GETLIST command</p>
<div class="line-block">
<div class="line">API endpoint | /files/getlist |</div>
</div>
<p><a href="#id31"><span class="problematic" id="id32">|---|</span></a>—|
| HTTP method  | PUT |
| Parameters   | transaction_id |
|              | user |
|              | group |
|              | tenancy |
|              | access_key |
|              | secret_key |
| Body         | JSON|
| Example      | <a href="#id9"><span class="problematic" id="id10">`</span></a>/files/getlist?transaction_id=1;user=”bob”;group=”root”;`|
| Body example | <a href="#id11"><span class="problematic" id="id12">`</span></a>{“filepath” : [“file1”, “file2”, “file3”]}`|</p>
<p>### DEL command</p>
<div class="line-block">
<div class="line">API endpoint | /files |</div>
</div>
<p><a href="#id33"><span class="problematic" id="id34">|---|</span></a>—|
| HTTP method  | DELETE |
| Parameters   | transaction_id |
|              | user |
|              | group |
|              | filepath |
|              | tenancy |
|              | access_key |
|              | secret_key |
| Body         | none |
| Example      | <a href="#id13"><span class="problematic" id="id14">`</span></a>/files/transaction_id=1;user=”bob”;group=”root”;filepath=”myfile.txt” <a href="#id15"><span class="problematic" id="id16">`</span></a><a href="#id17"><span class="problematic" id="id18">|</span></a></p>
<p>### DELLIST command</p>
<div class="line-block">
<div class="line">API endpoint | /files/dellist |</div>
</div>
<p><a href="#id35"><span class="problematic" id="id36">|---|</span></a>—|
| HTTP method  | PUT |
| Parameters   | transaction_id |
|              | user |
|              | group |
|              | tenancy |
|              | access_key |
|              | secret_key |
| Body         | JSON|
| Example      | <a href="#id19"><span class="problematic" id="id20">`</span></a>/files/dellist?transaction_id=1;user=”bob”;group=”root”<a href="#id21"><span class="problematic" id="id22">`</span></a>|
| Body example | <a href="#id23"><span class="problematic" id="id24">`</span></a>{“filepath” : [“file1”, “file2”, “file3”]}`|</p>
<p>## OAuth server</p>
<ul class="simple">
<li><p>Performs generation and authorisation of tokens</p></li>
<li><p>Currently JASMIN accounts portal.  Should be able to be something else as</p></li>
</ul>
<p>well.</p>
<p>The interaction of the NLDS client, NLDS server, OAuth server and the ingest of the Rabbit MQ queue is shown in Figure 2.</p>
<div class="line-block">
<div class="line">![client_server_seq](./uml/client_server_seq.png) |</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">-</dt>
<dd class="field-odd"><p></p></dd>
</dl>
<div class="line-block">
<div class="line"><strong>Figure 2</strong> Interaction of NLDS client, server, OAuth server and Rabbit MQ message broker. |</div>
</div>
<p># Rabbit MQ Exchange</p>
<p>The Rabbit MQ system consists of an Exchange, with a number of Topic Queues
with a subscriber to each topic queue.
These are:
* Work processor
* File / directory indexer / scanner
* Transfer processor
* Database processor
* Monitor</p>
<p>## Messaging</p>
<p>The NLDS relies on passing messages between different components in the
system.  These messages have to be formatted to match the receiving system and
so different message formats are used:</p>
<ol class="arabic simple">
<li><p>HTTP API / JSON</p></li>
<li><p>RabbitMQ</p></li>
<li><p>S3</p></li>
</ol>
<p>## Publishers and consumers</p>
<p>RabbitMQ has the concept of <em>Publishers</em>, which create messages and send them to
the exchange, and <em>Consumers</em>, which subscribe to a queue in the exchange, take
messages from the queue and processes them.</p>
<p>### Publishers
The publishing of messages occurs in the NLDS web server, which is implemented
in FastAPI. The <strong>put</strong>, <strong>get</strong> and <strong>getlist</strong> methods in the
<em>routing_methods.py</em> file all push messages to the RabbitMQ exchange using the
<strong>rabbit_publish_response</strong> method.  This uses a static instantiation of the
<strong>RabbitMQPublisher</strong> class.</p>
<div class="line-block">
<div class="line">![rabbit_publisher](./uml/rabbit_mq_publisher.png) |</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">-</dt>
<dd class="field-odd"><p></p></dd>
</dl>
<div class="line-block">
<div class="line"><strong>Figure 3.1</strong> RabbitMQPublisher class|</div>
</div>
<p>### Consumers</p>
<p>All of the NLDS processors inherit the RabbitMQConsumer, which in turn inherits
the RabbitMQPublisher class.</p>
<p>## Rabbit MQ Exchange Structure</p>
<div class="line-block">
<div class="line">![client_server_seq](./uml/queue_structure.png) |</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">-</dt>
<dd class="field-odd"><p></p></dd>
</dl>
<div class="line-block">
<div class="line"><strong>Figure 3.2</strong> Structure and interaction of Rabbit Queues.  Not all messages are shown.  For example, both <cite>Indexer 1</cite> and <cite>Indexer 2</cite> write <cite>work.index.complete</cite> messages to the <cite>Work Exchange</cite>.|</div>
</div>
<p>## Message flow</p>
<p>### Message flow for a <cite>putlist</cite> command
| ![message_flow_put1](./uml/message_flow_put_full.png) |
:-:
| <strong>Figure 4.1</strong> Flow of messages for a <cite>putlist</cite> case of transferring a list of files to the NLDS.|</p>
<p>### Message flow for a <cite>getlist</cite> command
| ![message_flow_get1](./uml/message_flow_get_full.png) |
:-:
| <strong>Figure 5.1</strong> Flow of messages for a <cite>getlist</cite> case of retrieving a list of files from the NLDS. Including the case where files are only on tape|</p>
<div class="line-block">
<div class="line">![message_flow_get2](./uml/message_flow_archive_put.png) |</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">-</dt>
<dd class="field-odd"><p></p></dd>
</dl>
<div class="line-block">
<div class="line"><strong>Figure 6.1</strong> Flow of messages for the internal archive worker to put unarchived files onto tape.|</div>
</div>
<p>## Message formats</p>
<p>Message content are in JSON format so as to aid human and machine readability.
The user entry point is the NLDS server, which presents a HTTP API
(REST-ful), implemented in FAST-API.  This HTTP API fulfills two different
classes of operations for NLDS: the CRUD (Create, Read, Update, Delete)
operations, and search operations.</p>
<p>## Inter-process communication</p>
<p>Communication between processes is carried out by submitting a RabbitMQ message
to the Exchange.  The  NLDS API server will submit the initial message into the
Exchange.  The RabbitMQ messages consist of a routing key and a JSON document
containing the data required to carry out the processes:</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>details {</dt><dd><p>transaction_id : &lt;string&gt;,
user           : &lt;string&gt;,
group          : &lt;string&gt;,
target         : &lt;string&gt;,
tenancy        : &lt;string&gt;,
access_key     : &lt;string&gt;,
secret_key     : &lt;string&gt;</p>
</dd>
</dl>
<p>},
meta {
},
data {
}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>All messages retain all parts of the <cite>details</cite> and <cite>meta</cite> fields in the JSON
message.  This allows the details of the transaction to be passed from process to
process, even when the process does not require some of the sub-fields in the
<cite>details</cite> or <cite>meta</cite> fields.  The <cite>data</cite> field can, and will, change between each
process.</p>
<p>The routing keys for the RabbitMQ messages have three components: the calling
application, the worker to act upon and the state or command for the worker.
The calling application part of the routing key will remain constant throughout
the operations lifecycle. This will allow multiple applications to use the
worker processes without interpreting messages destined for the other
applications.</p>
<p><cite>application.worker.state</cite></p>
<p>The <cite>data</cite> field contains the data required as input for the process and, after
processing, the output data.  For the messages detailed below in the
#WorkerProcesses section, this consists of a single key, pair:</p>
<blockquote>
<div><dl class="simple">
<dt>data<span class="classifier">{</span></dt><dd><p>filelist       : &lt;list&lt;(json,int)&gt;&gt;</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>The value for the <cite>filelist</cite> key is a <cite>&lt;list&gt;</cite> which can contain details for
multiple files.  Each <cite>&lt;list&gt;</cite> element is a <cite>PathDetails</cite> object, consisting of
a <cite>json</cite> document and a retry <cite>int</cite> (see the #retries section below).
The <cite>json</cite> document can contain a number of key / value pairs, some of which are
optional for each processor.  The <cite>json</cite> document looks like this:</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>file_details<span class="classifier">{</span></dt><dd><p>original_path       : &lt;str&gt;,
object_name         : &lt;str&gt;,
size                : &lt;int&gt;, (in kilobytes?)
user                : &lt;str&gt;, (get uid from LDAP?)
group               : &lt;str&gt;, (get gid from LDAP?)
file_permissions    : &lt;int&gt;, (unix file permissions)
access_time         : &lt;datetime&gt;, (timestamp of last accessed type)
filetype            : &lt;str&gt;, (LINK COMMON PATH, LINK ABSOLUTE PATH, DIRECTORY or FILE)
link_path           : &lt;str&gt;, (link position,path of link, related to either root or common path)</p>
</dd>
</dl>
<p>}
retries             : &lt;int&gt;,
retry_reasons       : &lt;list&lt;str&gt;&gt;</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>which mostly consists of the result of a stat call on the path/file in question,
as well as some useful metadata. The file type is included, which can be one of
a few options: FILE, DIRECTORY, LINK_COMMON_PATH, LINK_ABSOLUTE_PATH. The
distinction between these latter 2 options is whether the path of a given
symlink lies within the ‘common path’ of the files given in the transaction and
can therefore be stored as a relative link_path, or whether it lies outside of
the ‘common path’ and therefore must be stored as an absolute link_path. Note
that a symlink given as an absolute path to somewhere within the common path
should be stored as a LINK_COMMON_PATH - and therefore as a relative link_path -
and conversely a symlink given as a relative path to somewhere outside the
‘common path’ should be stored as an absolute LINK_ABSOLUTE_PATH.</p>
<p><cite>object_name</cite> in the above json document refers to the name of the object once
written to the object store.</p>
<p>### Applications</p>
<ul class="simple">
<li><p><cite>nlds-api</cite> - the calling API from the NLDS Fast API server</p></li>
<li><p><cite>gws-api</cite> - the calling API from the Group Workspace Scanner</p></li>
</ul>
<p>### Workers</p>
<ul class="simple">
<li><p><cite>nlds</cite> - the NLDS marshalling application.  Available only to the  <cite>nlds-api</cite></p></li>
</ul>
<p>API.
* <cite>index</cite> - the indexer, available to the <cite>nlds-api</cite> and <cite>gws-api</cite> APIs.
* <cite>transfer</cite> - the file transfer, from the disk system to object storage.
Available only to the  <cite>nlds-api</cite> API.</p>
<p>### State</p>
<p>These will vary between workers, but an example subset could be:</p>
<ul class="simple">
<li><p><cite>init</cite></p></li>
<li><p><cite>start</cite></p></li>
<li><p><cite>complete</cite></p></li>
</ul>
<p>### Key processing</p>
<p>RabbitMQ APIs for Python, such as Pika, can retrieve the key that has bound to
a queue, even if that key contains a wildcard (<cite>#</cite> or <cite>*</cite>).  The worker
processes use this capability to form the key for the return message, keeping
the same <cite>application</cite> portion of the key, but appending new <cite>worker</cite> and / or
<cite>state</cite> portions.</p>
<p>For example, consider two different scenarios.</p>
<p>1. the <cite>nlds-worker</cite> may issue the command <cite>nlds-api.index.start</cite> to
the Exchange.  This will bind to the <cite>index</cite> queue, which has the binding
<cite>#.index.*</cite>.  The <cite>Indexer</cite> process will parse the key, replacing the <cite>#</cite> part
with <cite>nlds-api</cite> and the <cite>*</cite> part with <cite>start</cite>.  From this the <cite>Indexer</cite> can form
the return key of <cite>nlds-api.index.complete</cite>.  This will bind to the <cite>nlds</cite>
queue and the <cite>nlds-worker</cite> will interpret this message.</p>
<p>2. an external application, the Group Workspace Scanner issues the command
<cite>gws-api.index.start</cite>.  This will, as before, bind to the <cite>index</cite> queue, and the
<cite>Indexer</cite> will parse the key.  This time, the return key will be
<cite>gws-api.index.complete</cite> and it will be left to the calling <cite>gws-api</cite> application
as to what happens next.  Note that there will be no queue in the NLDS system
that will bind to the key or interpret the message.</p>
<p>This is the mechanism that allows multiple applications to use parts of the NLDS
without consuming another application’s messages.</p>
<p>## Worker processes</p>
<p>The worker processes interact with each other via the Exchange and their topic
queues.  The <cite>NLDS</cite> worker acts as a marshalling process - i.e. it controls the
flow of the data through the system and knows which worker to send a message to
when another worker has finished.</p>
<p>### NLDS</p>
<p>This acts as a marshalling process.  Its first action, when a new
<cite>nlds-api.nlds.put</cite> message is consumed is to initiate the indexer with a
<cite>nlds-api.index.start</cite> message.
The NLDS Fast API server constructs the JSON from the parameters passed in the
URL.</p>
<p>#### —&gt; Input message</p>
<p><strong>Binding</strong> : <cite>nlds-api.nlds.put</cite></p>
<p><strong>Message</strong> :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>details<span class="classifier">{</span></dt><dd><p>transaction_id : &lt;string&gt;,
user           : &lt;string&gt;,
group          : &lt;string&gt;,
tenancy        : &lt;string&gt; (optional),
access_key     : &lt;string&gt;,
secret_key     : &lt;string&gt;</p>
</dd>
</dl>
<p>},
data : {</p>
<blockquote>
<div><p>filelist       : &lt;list&lt;(json,int)&gt;&gt;,</p>
</div></blockquote>
<p>},
meta : {</p>
<blockquote>
<div><p>label          : &lt;string&gt; (optional),
holding_id     : &lt;int&gt; (optional),
tag            : &lt;dict&gt; (optional)</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>The <cite>data : {filelist : }</cite> <cite>json</cite> documents contain :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>file_details<span class="classifier">{</span></dt><dd><p>original_path       : &lt;str&gt;,</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>#### &lt;— Output message</p>
<p><strong>Binding</strong> : <cite>nlds-api.index.start</cite></p>
<p><strong>Message</strong> :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>details<span class="classifier">{</span></dt><dd><p>transaction_id : &lt;string&gt;,
user           : &lt;string&gt;,
group          : &lt;string&gt;,
target         : &lt;string&gt; (optional),
tenancy        : &lt;string&gt; (optional),
access_key     : &lt;string&gt;,
secret_key     : &lt;string&gt;</p>
</dd>
</dl>
<p>},
data : {</p>
<blockquote>
<div><p>filelist       : &lt;list&lt;(json,int)&gt;&gt;</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>The <cite>data : {filelist : }</cite> <cite>json</cite> documents contain :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>file_details<span class="classifier">{</span></dt><dd><p>original_path       : &lt;str&gt;,</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>### Indexer</p>
<p>This takes the description of work that the Work Processor pushed onto the
queue and starts to build a file list.</p>
<p>At the end it can push two different messages to the queue:
* Index a directory
* Transfer a list of files from one data storage system to another</p>
<p>If a threshold number of files has been reached then it can:
* Push a message to transfer the files
* Push a message to index the remainder of the directories</p>
<p>The file indexer fulfills three purposes:</p>
<ol class="arabic simple">
<li><p>It ensures that the files that the user has supplied in a filelist are actually present.</p></li>
<li><p>It recursively indexes any directories that are in the filelist.</p></li>
<li><p>It splits the filelist into smaller batches to allow for restarting the transfer, asynchronicity of transfers and allow parallel transfers.</p></li>
</ol>
<p>This indexes the filelist by scanning the files to make sure they are present,
splitting up the filelist into manageable chunks and recursively scanning any
directories that are in the filelist.</p>
<p><cite>(optional)</cite> below indicates that the Indexer does not require those subfields to
operate.  However, it should echo back any subfields that occur in the <cite>details</cite>
field.</p>
<p><cite>(#)</cite> in a message below indicates that part of the key is matched to a single
word in the calling key and then the <cite>#</cite> in the return key is replaced with the
matched value.</p>
<p>#### —&gt; Input messages</p>
<p><strong>Binding</strong> : <cite>#.index.init</cite></p>
<p><strong>Message</strong> :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>details<span class="classifier">{</span></dt><dd><p>transaction_id : &lt;string&gt;,
user           : &lt;string&gt;,
group          : &lt;string&gt;,
target         : &lt;string&gt; (optional),
tenancy        : &lt;string&gt; (optional),
access_key     : &lt;string&gt;,
secret_key     : &lt;string&gt;</p>
</dd>
</dl>
<p>},
data : {</p>
<blockquote>
<div><p>filelist       : &lt;list&lt;(json,int)&gt;&gt;</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>The <cite>data : {filelist : }</cite> <cite>json</cite> documents contain :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>file_details<span class="classifier">{</span></dt><dd><p>original_path       : &lt;str&gt;,</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p><strong>Binding</strong> : <cite>#.index.start</cite></p>
<p><strong>Message</strong> :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>details<span class="classifier">{</span></dt><dd><p>transaction_id : &lt;string&gt;,
user           : &lt;string&gt;,
group          : &lt;string&gt;
target         : &lt;string&gt; (optional),
tenancy        : &lt;string&gt; (optional),
access_key     : &lt;string&gt;,
secret_key     : &lt;string&gt;</p>
</dd>
</dl>
<p>},
data : {</p>
<blockquote>
<div><p>filelist       : &lt;list&lt;(json,int)&gt;&gt;</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>The <cite>data : {filelist : }</cite> <cite>json</cite> documents contain :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>file_details<span class="classifier">{</span></dt><dd><p>original_path       : &lt;str&gt;,</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>#### &lt;— Output messages</p>
<p><strong>Binding</strong> : <cite>(#).index.start</cite></p>
<p><strong>Message</strong> :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>details<span class="classifier">{</span></dt><dd><p>transaction_id : &lt;string&gt;,
user           : &lt;string&gt;,
group          : &lt;string&gt;,
target         : &lt;string&gt; (optional),
tenancy        : &lt;string&gt; (optional),
access_key     : &lt;string&gt;,
secret_key     : &lt;string&gt;</p>
</dd>
</dl>
<p>},
data : {</p>
<blockquote>
<div><p>filelist       : &lt;list&lt;(json,int)&gt;&gt;</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>The <cite>data : {filelist : }</cite> <cite>json</cite> documents contain :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>file_details<span class="classifier">{</span></dt><dd><p>original_path       : &lt;str&gt;,</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p><strong>Binding</strong> : <cite>(#).index.complete</cite></p>
<p><strong>Message</strong> :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>details<span class="classifier">{</span></dt><dd><p>transaction_id : &lt;string&gt;,
user           : &lt;string&gt;
group          : &lt;string&gt;
target         : &lt;string&gt; (optional),
tenancy        : &lt;string&gt; (optional),
access_key     : &lt;string&gt;,
secret_key     : &lt;string&gt;</p>
</dd>
</dl>
<p>},
data : {</p>
<blockquote>
<div><p>filelist       : &lt;list&lt;(json,int)&gt;&gt;</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>The <cite>data : {filelist : }</cite> <cite>json</cite> documents contain :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>file_details<span class="classifier">{</span></dt><dd><p>original_path       : &lt;str&gt;,
size                : &lt;int&gt;,
user                : &lt;str&gt;,
group               : &lt;str&gt;,
file_permissions    : &lt;int&gt;,
access_time         : &lt;datetime&gt;,
filetype            : &lt;str&gt;,
link_path           : &lt;str&gt;,</p>
</dd>
</dl>
<p>}
retries         : &lt;int&gt;,
retry_reasons   : &lt;list&lt;str&gt;&gt;</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>### Failure Modes for indexing</p>
<ul class="simple">
<li><p>Files not found</p></li>
<li><p>Disk not available</p></li>
<li><p>User does not have permissions to access files</p></li>
</ul>
<p>… add to these</p>
<p>## Transfer processor
This takes the list of files from the File Indexer and transfers them from
one storage medium to another
At the end it pushes a message to the queue to say it has completed.
Needs the access key and secret key in the message.</p>
<p>Asynchronicity of the transfers is a desirable byproduct of the indexer splitting
the filelist into smaller batches.  It also allows for parallel transfer, with
multiple transfer workers.  Finally, if a transfer worker fails, and does not
return an acknowledgement message to the Exchange, the message will be sent out
again, after a suitable timeout period.</p>
<p>#### —&gt; Input messages</p>
<p><strong>Binding</strong> : <cite>nlds-api.transfer-put.start</cite></p>
<p><strong>Message</strong> :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>details<span class="classifier">{</span></dt><dd><p>transaction_id : &lt;string&gt;,
user           : &lt;string&gt;,
group          : &lt;string&gt;,
target         : &lt;string&gt; (optional),
tenancy        : &lt;string&gt; (optional),
access_key     : &lt;string&gt;,
secret_key     : &lt;string&gt;</p>
</dd>
</dl>
<p>},
data : {</p>
<blockquote>
<div><p>filelist       : &lt;list&lt;(json,int)&gt;&gt;</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>The <cite>data : {filelist : }</cite> <cite>json</cite> documents contain :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>file_details<span class="classifier">{</span></dt><dd><p>original_path       : &lt;str&gt;,
size                : &lt;int&gt;,
user                : &lt;str&gt;,
group               : &lt;str&gt;,
file_permissions    : &lt;int&gt;,
access_time         : &lt;datetime&gt;,
filetype            : &lt;str&gt;,
link_path           : &lt;str&gt;,</p>
</dd>
</dl>
<p>}
retries             : &lt;int&gt;,
retry_reasons       : &lt;list&lt;str&gt;&gt;</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>#### &lt;— Output messages</p>
<p><strong>Binding</strong> : <cite>nlds-api.transfer-put.complete</cite></p>
<p><strong>Message</strong> :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>details<span class="classifier">{</span></dt><dd><p>transaction_id : &lt;string&gt;,
user           : &lt;string&gt;,
group          : &lt;string&gt;,
target         : &lt;string&gt; (optional),
tenancy        : &lt;string&gt; (optional),
access_key     : &lt;string&gt;,
secret_key     : &lt;string&gt;</p>
</dd>
</dl>
<p>},
data : {</p>
<blockquote>
<div><p>filelist       : &lt;list&lt;(json,int)&gt;&gt;</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>The <cite>data : {filelist : }</cite> <cite>json</cite> documents contain :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>file_details<span class="classifier">{</span></dt><dd><p>original_path       : &lt;str&gt;,
object_name         : &lt;str&gt;,
size                : &lt;int&gt;,
user                : &lt;str&gt;,
group               : &lt;str&gt;,
file_permissions    : &lt;int&gt;,
access_time         : &lt;datetime&gt;,
filetype            : &lt;str&gt;,
link_path           : &lt;str&gt;,</p>
</dd>
</dl>
<p>}
retries         : &lt;int&gt;,
retry_reasons   : &lt;list&lt;str&gt;&gt;</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>### Failure Modes for transfer</p>
<ul class="simple">
<li><p>Files not found (files have disappeared since indexing)</p></li>
<li><p>Object store not available</p></li>
<li><p>User does not have permissions to access files (permissions have changed</p></li>
</ul>
<p>since indexing)</p>
<p>## Catalogue processor
Add or retrieve files and metadata to a file catalogue database.</p>
<p>Operations:
* <cite>put</cite> - write a file record to the catalogue.  One file record at once.
* <cite>get</cite> - read (a) file record(s) from the catalogue.  Allow basic matching of
directories, e.g. <cite>get /gws/nopw/j04/cedaproc/nrmassey/OxPEWWES2/</cite> will get all
the files under the <cite>OxPEWWES2</cite> record.</p>
<p>This requires a Database schema to store, these are contained in the
<cite>&lt;file_details&gt;</cite> <cite>json</cite> document:</p>
<ul class="simple">
<li><p>File path</p></li>
<li><p>Object store path</p></li>
<li><p>Size</p></li>
<li><p>User</p></li>
<li><p>Group</p></li>
<li><p>Unix permissions</p></li>
<li><p>Last accessed timestamp</p></li>
<li><p>Filetype (LINK COMMON PATH, LINK ABSOLUTE PATH, DIRECTORY or FILE)</p></li>
<li><p>Link position (path of link, related to either root or common path)</p></li>
</ul>
<p>### Database on disk or object store
We could store the information about the files on disk or on the object store,
without requiring a database.
These files could be stored alongside the objects in the bucket named after the
transaction id.
The name of the file would be the hash of the file path and should (probably) be
in JSON format. e.g.</p>
<blockquote>
<div><p>&lt;tenancy&gt;/&lt;transaction_id&gt;/&lt;file path hash&gt;.json</p>
</div></blockquote>
<p>#### Likely user interactions for <strong>putting</strong> a file:</p>
<p>1.  Put a filelist (no tags).  System to generate a sequential batch id.  Store
filelist and batch id with transaction id.
2.  Put a filelist with tags.  System to generate a sequential batch id, and
assign the tags.  Store filelist, tags and batch id with transaction id.</p>
<p>#### Likely user interactions for <strong>getting</strong> a file.</p>
<p>1.  Get a filelist by filepaths.  System has to determine which transaction
id(s) the files in the filelist belong to.  There may be many transaction ids -
as many as files in the filelist.
2.  Get a filelist by tag.  System has to determine which transaction id(s) the
tags are referring to.  There may be many transaction ids, as a tag can be
applied across transaction ids / batches.
3.  Get a filelist by sequential batch id.  There is a direct one to one mapping
between batch ids and transaction ids.  System has to determine which
transaction id
4.  Get a filelist by transaction id.  This is the easiest, but least likely to
be used scenario.</p>
<p>### Database on PostgreSQL server</p>
<p>The boring / safe option</p>
<p>### SQLlite database on Object Store</p>
<p>## Retry mechanism
At any stage in the transaction, one component of the storage hierarchy may not
be available.  This could be during a PUT, where the disk that the user’s files
reside on is not available, or the object storage target may not be available.
During a GET, the object storage may not be available, or the user’s disk may
be full or not available.  In these cases, a <em>retry mechanism</em> is needed.</p>
<p>In the messages for indexing and transfers above, there is the part that records
the filenames:</p>
<blockquote>
<div><dl class="simple">
<dt>data {</dt><dd><p>filelist       : &lt;list&lt;json,int&gt;&gt;</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>Here, the <cite>&lt;json&gt;</cite> part of the message records the full details of the file, and
the <cite>&lt;int&gt;</cite> part records the number of times an attempt has been made to index
or transfer the file.  These are stored internally in NLDS as part of the
<cite>PathDetails</cite> object:</p>
<blockquote>
<div><dl class="simple">
<dt>PathDetails:</dt><dd><p>file_details:   (described above),
retries:        &lt;int&gt;,
retry_reasons:  &lt;list&lt;str&gt;&gt;</p>
</dd>
</dl>
</div></blockquote>
<p>When a task is performed, such as indexing or transferring a file, which
subsequently fails, the PathDetails object is taken out of the return filelist
and put into a failed filelist where the retries count is incremented by 1 and
the reason for the failure is added to <cite>retry_reasons</cite>.  A message is formed
with this failed filelist as the <cite>data</cite> part of the message, and the <cite>details</cite>
part of the message is the same as the originating message.  Currently, this
message is passed back to the exchange, with the same routing key as the
originating message.</p>
<p>Once an item in the filelist has exceeded a server-configured number of
retries, it will be permanently failed and a message will be passed back to the
user (how?) telling them of this.</p>
<p>Just passing back the message immediately is not optimal.  The task will have
failed for a reason which may not have been fixed in the (potentially and
probably) short time that it will take for the message to be submitted to the
exchange and then consumed by the task that had initially failed. With such a
short duration between failing and then trying to complete the task, it is
likely that the task will fail again. To overcome this we have implemented the
option to use the [delayed exchange plugin](<a class="reference external" href="https://github.com/rabbitmq/rabbitmq-delayed-message-exchange/">https://github.com/rabbitmq/rabbitmq-delayed-message-exchange/</a>) for RabbitMQ which allows messages to
request to be delayed before being routed by the exchange, via a variable
(<cite>x-delay</cite>) specified in the message header.</p>
<p>These delays increase exponentially with increasing retry count, with a default
configuration of:</p>
<ul class="simple">
<li><p>First failure, resend immediately</p></li>
<li><p>Second failure, wait 30 seconds</p></li>
<li><p>Third failure, wait 1 minute</p></li>
<li><p>Fourth failure, wait 1 hour</p></li>
<li><p>Fifth failure, wait 1 day</p></li>
<li><p>Sixth failure, wait 5 days</p></li>
</ul>
<p>Subsequent retries will continue to wait 5 days, until the maximum retry limit
is reached (default = 5).</p>
<p>## Monitoring
Important!
How do we know when a transfer has completed, if it has been split into
multiple components?</p>
<p>## Logging
Logging is distinct from [Monitoring](#Monitoring):</p>
<ul class="simple">
<li><p>Monitoring is a service provided for the user to provide details of how their</p></li>
</ul>
<p>transfer is progressing.
* Logging is a service provided to the system administrators to provide details
of how well the system is working.</p>
<p>The NLDS is deployed via Docker containers and each processor will be situated in
a container.  There may be multiples of a processor type in separate containers,
and, at the very least, a container for each processor.  For example, a deployment
may consist of:</p>
<ul class="simple">
<li><p>A container with the NLDS server in it.</p></li>
<li><p>A container with the Monitoring process in it.</p></li>
<li><p>Two containers with the Indexer processors in them.</p></li>
<li><p>Three containers with the Transfer processors in them.</p></li>
<li><p>A container with the Catalog process in it.</p></li>
</ul>
<p>In this scenario there are two bad ways of logging:</p>
<p>1. Print messages to stdout on each container - you would have to monitor each
container in realtime to determine any error.
2. Using the Python logging module to output logs to a file.  In a containerised
environment, this will result in a separate log file written in each container.
To determine the error, you would have to log into each container and examine the
logs.</p>
<p>Instead, a better system is to use the Rabbit MQ exchange to pass log messages
to a Logging process, which is in a container.  Logs can be written to the
container from the log messages passed via the exchange, with the advantage that
all the logs are colocated, and that multiple containers of the same processor
can write to the same log file.  This system also allows a dashboard to be built
/ used to monitor the processors and their logs.</p>
<p>### Strategy
1.  Each processor should have its own log file: e.g. nlds_index.log,
nlds_transfer.log, nlds_index.log, nlds_catalog.log, nlds_log.log etc.
2.  The processors pass log messages to the Rabbit MQ exchange using the key
<cite>#.log.*</cite>.  For NLDS this will be <cite>nlds.log.write</cite> to write a log message.
3.  The Logging processor subscribes to the logging topic queue, receives the
messages and writes the log files.
4.  Logrotate should be used to manage the logs.
5.  Separate levels of logging should be permitted, e.g. <strong>DEBUG, INFO,
WARNING, ERROR, CRITICAL</strong></p>
<blockquote>
<div><ul class="simple">
<li><p><strong>DEBUG</strong> - information provided when debugging / in development.  Very</p></li>
</ul>
<p>verbose - e.g. now I’m going to do this particular operation and I’m going
to tell you all about it.
* <strong>INFO</strong> - information provided when in production. Changes of state, etc.
Not as verbose as <strong>DEBUG</strong> but still informative.
* <strong>WARNING</strong> - something wasn’t optimum, but I can recover from it and
continue the operation.
* <strong>ERROR</strong> - something has prevented me from completing the operation but
has left the system / processor up and running.
* <strong>CRITICAL</strong> - something so bad has happened that the system / processor
has had to exit.</p>
</div></blockquote>
<ol class="arabic simple" start="6">
<li><p>Use exceptions to trap <strong>WARNING, ERROR</strong> and <strong>CRITICAL</strong> levels.</p></li>
</ol>
<p>### Message format</p>
<p><strong>Binding</strong> : <cite>(#).log.write</cite></p>
<p><strong>Message</strong> :</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl class="simple">
<dt>details {</dt><dd><p>transaction_id : &lt;string&gt;,
user           : &lt;string&gt;,
group          : &lt;string&gt;,
target         : &lt;string&gt; (optional),</p>
</dd>
</dl>
<p>},
data {</p>
<blockquote>
<div><p>container_id   : &lt;string&gt;,
processor_id   : &lt;string&gt;,
module_name    : &lt;string&gt;,
code_line      : &lt;string&gt;,
message        : &lt;string&gt;</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p><cite>code_line</cite> and <cite>module_name</cite> can be derived from the exception using the
<cite>traceback</cite> module.</p>
<p># User interaction via the NLDS client / client API</p>
<p>User actions:</p>
<p>## Transfer actions
### PUT a single / list of files</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>put</cite></p></li>
<li><p><cite>putlist</cite></p></li>
<li><p>Required arguments
- <cite>–user</cite>
- <cite>–group</cite>
- <cite>filepath|filelist</cite></p></li>
<li><p>Optional
- <cite>–label=</cite>: if a holding with label exists then add to an existing holding with the label, otherwise create a new holding with the label
- <cite>–holding_id=</cite>: adds to an existing holding with the (integer) id
- <cite>–tag=key:value</cite>: adds a tag to the holding on PUT</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>### GET a single / list of files</dt><dd><ul class="simple">
<li><p><cite>get</cite></p></li>
<li><p><cite>getlist</cite></p></li>
<li><p>Required arguments
- <cite>–user=</cite>
- <cite>–group=</cite>
- <cite>–target=</cite>
- <cite>filepath|filelist</cite></p></li>
<li><p>Optional
- <cite>–label=</cite>: get the file from a holding with matching label
- <cite>–holding_id=</cite>: get the file from a holding with the (integer) id
- <cite>–tag=key:value</cite>: get the file from a holding with the matching tag</p></li>
</ul>
</dd>
<dt>### DELETE a single / list of files</dt><dd><ul class="simple">
<li><p><cite>del</cite></p></li>
<li><p><cite>dellist</cite></p></li>
<li><p>Required arguments
- <cite>–user=</cite>
- <cite>–group=</cite>
- <cite>filepath|filelist</cite></p></li>
</ul>
</dd>
</dl>
<p>## Query actions</p>
<dl>
<dt>### List the holdings for a user / group</dt><dd><ul class="simple">
<li><p><cite>list</cite></p></li>
<li><p>Required arguments
- <cite>–user=</cite></p></li>
<li><p>Optional arguments
- <cite>–group=</cite>
- <cite>–holding_id=</cite> (integer)
- <cite>–tag=key:value</cite> (filter by tag)
- <cite>–label=</cite> (filter by label)
- <cite>–time=datetime|(start datetime, end datetime)</cite> (time the files were ingested)</p></li>
</ul>
</dd>
<dt>### List the files for a user / group</dt><dd><ul class="simple">
<li><p><cite>find</cite></p></li>
<li><p>Required arguments
- <cite>–user=</cite></p></li>
<li><p>Optional arguments
- <cite>–group=</cite>
- <cite>–holding_id=</cite> (integer)
- <cite>–tag=key:value</cite> (filter by tag for the holding containing the files)
- <cite>–label=</cite> (filter by the holding label)
- <cite>–time=datetime|(start datetime, end datetime)</cite> (time the files were ingested)
- <cite>–path=</cite> (filter by original path, can be a substring, regex or wildcard)</p></li>
</ul>
</dd>
<dt>### Update the holding metadata</dt><dd><ul>
<li><p><cite>meta</cite></p></li>
<li><p>Required arguments
- <cite>–user=</cite>
- One of (must guarantee uniqueness)</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>–holding_id=</cite></p></li>
<li><p><cite>–label=</cite></p></li>
</ul>
</div></blockquote>
<ul class="simple">
<li><p>Optional
- <cite>–group</cite>
- <cite>–update_tag=key:value</cite> (create or amend a tag)
- <cite>–update_label=</cite> (change the label)</p></li>
</ul>
</li>
</ul>
</dd>
</dl>
<p># Development notes</p>
<p>## Indexer
Sym-links and directories need to be preserved in the backup, this is still
being worked out but will need implementing! Symlinks with common path - i.e.
that point to a file/directory within the scope of the original batch - need to
be converted to be relative so that restoring the files in a different directory
structure works. Similarly, symlinks to locations not on the common path need to
be preserved as absolute links.</p>
<p>## Monitoring
Database for monitoring is quite simple (I think), we only require:
- transaction_id
- holding_id (when one exists)
- tag (if one exists)
- state
- user
- message_splits</p>
<p>we can put this in a table called transaction_states</p>
<p>#### What states do we want?
We probably want, at least:
1. Routing (sent from API-server?)
2. Splitting
3. Indexing
4. Transferring
5. Cataloging
6. Complete
7. Failed (just thought of this!)</p>
<p>#### What to do about message splitting
Given that messages can be split into batches and by size in the indexer, we could have a single transaction be split across N jobs where N &gt;&gt; 1. This is a worst case obviously, but it is absolutely necessary to be prepared for it.</p>
<p>We can keep track of how many splits have occurred in the indexer, both at the ‘split’ part and at the ‘index’ part. We could have a <cite>message_split_count</cite>  as part of the database holding. We could in fact have a second table of transaction_splits with a 1-to-many relationship and have each of these have a separate state, but this feels a little onorous - we’re trying to have as little state as possible and this would require each individual message to keep track of how many times it has split. Not to mention the splits table could get VERY big VERY quickly - we would need to purge it basically 2 weeks after we’re sure that a transaction has completed,</p>
<p>What we could do instead is have some table which is states, and that can keep track of how many times each state has been pinged for each transaction. For example, take a message that has been split 6 times, we would then have a state_count for each of the states that is incremented when a new state-update message comes in. Then we know a transaction has completely finished when a state_count == the split count. This is potentially problematic though as it could lead to a race condition - if two messages arrive at a very similar time and try to increment the counter at the same time then we could lose a count and the job would forever be stuck in ‘not enough counts for X state’.</p>
<p>Another alternative is to have the state be updated as soon as a _new_ state comes in. This gets around race conditions as it makes the overall database value matter less. Two messages arriving with conflicting states could race but the state is essentially ratcheting up - the later state will come out on top eventually. In fact all jobs will reach a COMPLETE state eventually as the final job going into the COMPLETE state will not have anything else to compete against. However, the state could therefore be unrepresentative of the actual state of the system.</p>
<p>We could get around race conditions entirely by requiring that there only be one monitoring queue with a prefetch of 1. Therefore there will only ever be one read to the database at a time and it will definitely be serial. This might be a bottleneck though, especially if we’re reading and writing to the database multiple times for each transaction at each step of the process. Say we have 1000 transactions, each split into a 1000 subjobs, each writing to the database at each stage of the put process. There would be $5times1000times1000 = 5,000,000$ individual database writes just for that clump of jobs.</p>
<p>I think, on balance, this volume of writes probably isn’t a problem (can discuss with Neil obviously) so I’m going to press ahead with a design which has both a ratcheting job state and individual transaction_state_counts, with a queue-prefetch of 1. Therefore we want a single table with the following:
1. id [INT]
2. transaction_id [UUID | STRING] (unique)
3. user [STRING] (uid?)
4. group [STRING] (gid?)
5. (ratcheted) state [ENUM | INT]
6. split_count [INT] (set at split &amp; index step)
9. indexing_count [INT]
10. transferring_count [INT]
11. cataloging_count [INT]
12. complete_count [INT]
13. failed_count [INT]</p>
<p>with the transaction_id either given, or gotten from a tag/holding_id from the catalog if requesting that way. Note that requesting by tag will therefore not work if the job has not reached the cataloging step yet, although a job will not have</p>
<p>An open question remains about timestamps - do we need one? The ingest time will be saved for any given catalog record, but reording the timestamp of most recent change for the furthest state might be useful? The whole idea of this is to be as slim as possible though, so it might be best to just leave it out given it doesn’t provide that much benefit.</p>
<p>### On database visibility for monitoring requests
Neil made the good point that having the api-server talk directly to the databases might be bad for the database if traffic levels get too high, i.e. the server or the database would be in danger of clogging up the whole system. A better approach would be to have the API-server be a consumer with its own queue, and use an asynchronous <cite>await</cite> command from within a given ‘router’ function block (i.e. one of the router endpoints ) to ensure that the information can be sent back to the user with the API http response. We need to do some figuring-out to work out if this is possible.</p>
<p><strong>Initial thoughts:</strong>
- The awaiting thread will need to be picked back up by the consumer callback on the API-server - I am not sure how this works!
- Could call an async <cite>send_and_receive</cite> function within the router function which creates a consumer, sends a message to the exchange and then waits for a response (for some timeout value…)</p>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="home.html" class="btn btn-neutral float-left" title="CEDA Near-Line Data Store" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="system-status.html" class="btn btn-neutral float-right" title="The System Status Monitor" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022-2024, Neil Massey &amp; Jack Leland.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>